---
layout: single
title:  "하이퍼파라미터"
categories: data
#검색이 안되고 싶을 때
#search: false
---

## 1. 하이퍼파라미터와 파라미터

<table border="1">
  <tr>
    <th colspan="4">Hyperparameter vs Parameter</th>  <!-- 가로 3칸 병합 -->
  </tr>
  <tr>
    <th>구분</th>
    <th>Hyperparameter</th>
    <th>Parameter</th>
  </tr>
  <tr>
    <td>정의</td>
    <td>모델의 구성 요소이자 데이터로부터 학습되는 것</td>
    <td>모델 학습 과정에 반영되며, 학습을 시작하기 전에 미리 값을 결정하는 것</td>
  </tr>
  <tr>
    <td>예시</td>
    <td>뉴런의 가중치(weights, bias)</td>
    <td>뉴런의 개수, 계층의 수</td>
  </tr>
</table>  

즉, '하이퍼파라미터'는 학습 전 사용자가 조정하는 값, '파라미터'는 결과 값이다.

## 2. 하이퍼파라미터 종류
주로 사용하는 하이퍼파라미터는 모델의 유형과 학습 과정에 따라 달라지지만, 일반적으로 사용되는 하이퍼파라미터는 아래와 같다.

1) 학습률 (Learning Rate) 
: 모델이 매번 학습할 때 가중치 업데이트의 크기를 결정
* 너무 크면 최적화가 빨리 끝나고, 너무 작으면 학습 속도가 너무 느려짐

2) 배치 크기 (Batch Size)
: 한 번에 모델에 입력되는 데이터 샘플의 수
* 작은 배치는 더 세밀하게 가능하지만 계산 속도가 느려질 수 있음

3) 에포크 (Epoch)
: 전체 훈련 데이터를 한 번 학습하는 횟수
* 에포크 수가 너무 많으면 과적합(overfitting) 될 수 있고, 너무 작으면 모델이 제대로 학습되지 않을 수 있음

4) 은닉층 수 (Number of Hidden Layers)
: 신경망 모델에서 입력층과 출력층 사이에 존재
* 은닉층이 많을수록 모델이 더 복잡해지며, 과적합을 유발할 수 있음

5) 은닉층 크기 (Number of Neurons in Hidden Layers)
: 각 은닉층에 포함된 뉴런의 수
* 너무 적은 뉴런은 모델의 학습 능력이 부족하고, 너무 많은 뉴런은 계산 비용이 커지며 과적합 유발할 수 있음

6) 활성화 함수 (Activation Function)
: 각 뉴런의 출력값을 계산할 때 사용하는 함수
* 선택한 활성화 함수에 따라 모델의 성능과 학습 속도가 달라짐

7) 정규화 (Regularization)
: 과적합을 방지하기 위해 가중치 크기를 제한하는 방법
* 모델이 너무 복잡해지지 않도록 함

8) 드롭아웃 (Dropout Rate)
: 학습 중 일부 뉴런을 랜덤하게 제거하여 과적합을 방지하는 기법
* 보통 0.2~0.5 범위로 설정되며, 너무 큰 값은 학습이 불안정해질 수 있음

9) 모멘텀 (Momentum)
: 이전 기울기를 반영하여 학습을 가속화하는 방법
* 작은 값은 더 느린 수렴을 유도하고 너무 큰 값은 불안정한 학습 초래 (0.9~0.99 범위로 설정)

10) 학습률 감소 (Learning Rate Decay)
: 학습 과정이 진행됨에 따라 학습률을 점진적으로 감소시키는 방법
* 초기 학습률은 크고, 학습이 진행됨에 따라 점차 감소시켜 더 세밀한 조정

이 외에도 모델마다 다양한 하이퍼파라미터가 존재하며,
가장 좋은 성능을 내는 파라미터를 찾는 과정을 '하이퍼파라미터 튜닝'이라고 한다.

## 3. 하이퍼파라미터 튜닝

하이파라미터 튜닝이란, 모델을 최적화하기 위해 하이퍼파라미터를 조정하는 과정이다.  
하이퍼파라미터 튜닝은 모델의 성능을 최대화하고, 과적합을 방지하는 데 중요하다.  

---

지금까지 하이퍼파라미터에 대해 알아보았다. 
하이퍼파라미터의 튜닝 기법들도 다양하던데 이것도 올려봐야겠다!





